{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCgfi8oMQ3B2s9jVjmDf0B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhanavathAkhil/AI-project/blob/main/code/AI_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')  # Ensure tokenizer also works\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5ZvzVo0axsz",
        "outputId": "a18cf83f-1b56-481b-a234-9efa061ab9bd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import os\n",
        "\n",
        "nltk_data_path = \"/usr/local/nltk_data\"\n",
        "\n",
        "# Ensure directory exists\n",
        "os.makedirs(nltk_data_path, exist_ok=True)\n",
        "\n",
        "# Download stopwords to the correct directory\n",
        "nltk.download('stopwords', download_dir=nltk_data_path)\n",
        "\n",
        "# Append the path manually so NLTK can find the data\n",
        "nltk.data.path.append(nltk_data_path)\n",
        "\n",
        "# Test stopwords again\n",
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('english')[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWIIw20Pa02q",
        "outputId": "f150d6d1-f6dc-44b5-aa7f-ac0eb412eaca"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /usr/local/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Import Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# ‚úÖ Fix NLTK Issues\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# ‚úÖ Load Dataset with Encoding Fix\n",
        "file_path = \"/content/drive/My Drive/AI/data.csv\"  # Adjust path accordingly\n",
        "df = pd.read_csv(file_path, encoding=\"ISO-8859-1\")\n",
        "\n",
        "# ‚úÖ Show Dataset Information\n",
        "print(\"‚úÖ Dataset loaded successfully!\")\n",
        "print(\"üìå Available Columns:\", df.columns)\n",
        "\n",
        "# ‚úÖ Automatically Identify Text & Label Columns\n",
        "possible_text_cols = ['text', 'article_content', 'content']\n",
        "possible_label_cols = ['label', 'labels', 'category']\n",
        "\n",
        "text_column = next((col for col in df.columns if col in possible_text_cols), df.columns[0])\n",
        "label_column = next((col for col in df.columns if col in possible_label_cols), df.columns[1])\n",
        "\n",
        "print(f\"üìå Using Text Column: {text_column}\")\n",
        "print(f\"üìå Using Label Column: {label_column}\")\n",
        "\n",
        "# ‚úÖ Load Stopwords\n",
        "try:\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# ‚úÖ Text Preprocessing Function\n",
        "def preprocess_text(text):\n",
        "    text = str(text).lower().strip()  # Convert to lowercase & remove extra spaces\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    words = tokenizer.tokenize(text)\n",
        "    words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
        "\n",
        "    return \" \".join(words) if words else np.nan  # Return NaN if empty\n",
        "\n",
        "# ‚úÖ Apply Preprocessing\n",
        "df['clean_text'] = df[text_column].apply(preprocess_text)\n",
        "\n",
        "# ‚úÖ Convert Labels to Binary if Needed\n",
        "if df[label_column].dtype == 'object':\n",
        "    df['label'] = df[label_column].astype('category').cat.codes  # Convert categorical labels\n",
        "else:\n",
        "    df['label'] = df[label_column]\n",
        "\n",
        "# ‚úÖ Remove Empty or NaN Rows After Preprocessing\n",
        "df.dropna(subset=['clean_text'], inplace=True)\n",
        "\n",
        "print(f\"üìå Total Records After Preprocessing: {len(df)}\")\n",
        "if df.empty:\n",
        "    print(\"‚ùå Error: No valid text data found after preprocessing! Exiting...\")\n",
        "    exit()\n",
        "\n",
        "# ‚úÖ Split Data into Training & Testing Sets\n",
        "try:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(df['clean_text'], df['label'], test_size=0.2, random_state=42)\n",
        "except ValueError as e:\n",
        "    print(f\"‚ùå Error in train-test split: {e}\")\n",
        "    exit()\n",
        "\n",
        "print(\"‚úÖ Sample preprocessed texts:\\n\", X_train.head())\n",
        "\n",
        "# ‚úÖ Convert Text to Numerical Representation using TF-IDF\n",
        "vectorizer = TfidfVectorizer(min_df=1, max_features=5000)\n",
        "try:\n",
        "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "    X_test_tfidf = vectorizer.transform(X_test)\n",
        "except ValueError as e:\n",
        "    print(f\"‚ùå Error: {e}\")\n",
        "    print(\"üîπ Possible reason: The vocabulary is empty due to excessive stopword removal.\")\n",
        "    exit()\n",
        "\n",
        "# ‚úÖ Train a Na√Øve Bayes Model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# ‚úÖ Predictions\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "# ‚úÖ Evaluate Model Performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"‚úÖ Model Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nüîπ Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# ‚úÖ Example Prediction\n",
        "sample_text = \"Breaking news: Stock market crashes due to economic instability!\"\n",
        "sample_text = preprocess_text(sample_text)\n",
        "sample_text_tfidf = vectorizer.transform([sample_text])\n",
        "prediction = model.predict(sample_text_tfidf)\n",
        "\n",
        "if prediction[0] == 1:\n",
        "    print(\"üõë Fake News Detected!\")\n",
        "else:\n",
        "    print(\"‚úÖ Real News!\")\n",
        "\n",
        "# ‚úÖ Example Prediction\n",
        "sample_text = \"narendra modi was removed from prime minister\"\n",
        "sample_text = preprocess_text(sample_text)\n",
        "sample_text_tfidf = vectorizer.transform([sample_text])\n",
        "prediction = model.predict(sample_text_tfidf)\n",
        "\n",
        "if prediction[0] == 1:\n",
        "    print(\"üõë Fake News Detected!\")\n",
        "else:\n",
        "    print(\"‚úÖ Real News!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdfYrzuGVkuw",
        "outputId": "49cbef30-36e4-4aba-986a-4d42c1761fe3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset loaded successfully!\n",
            "üìå Available Columns: Index(['unit_id', 'article_title', 'article_content', 'source', 'date',\n",
            "       'location', 'labels'],\n",
            "      dtype='object')\n",
            "üìå Using Text Column: article_content\n",
            "üìå Using Label Column: labels\n",
            "üìå Total Records After Preprocessing: 804\n",
            "‚úÖ Sample preprocessed texts:\n",
            " 344    published august send httpsabahdailyesxnam lea...\n",
            "350    published april syrian opposition forces sunda...\n",
            "443    april zarif urges intl factfinding mission pro...\n",
            "331    oct russian jets pounded several oppositionhel...\n",
            "290    last updated aug beirut syrian russian warplan...\n",
            "Name: clean_text, dtype: object\n",
            "‚úÖ Model Accuracy: 0.5901\n",
            "\n",
            "üîπ Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.25      0.35        72\n",
            "           1       0.59      0.87      0.70        89\n",
            "\n",
            "    accuracy                           0.59       161\n",
            "   macro avg       0.59      0.56      0.53       161\n",
            "weighted avg       0.59      0.59      0.54       161\n",
            "\n",
            "‚úÖ Real News!\n",
            "üõë Fake News Detected!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "85XtuVJkdNHD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}